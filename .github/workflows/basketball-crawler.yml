name: BasketballBund Crawler

on:
  schedule:
    # Run every second night at 2:00 AM (same as original crawler)
    - cron: '0 2 */2 * *'
  workflow_dispatch:
    # Allow manual triggering

jobs:
  crawl:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Cache dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('crawler/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
          
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r crawler/requirements.txt
        
    - name: Run crawler
      env:
        SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
        SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
        LEAGUE_ID: ${{ secrets.LEAGUE_ID }}
      working-directory: ./crawler
      run: |
        python main.py
        
    - name: Notify on failure
      if: failure()
      run: |
        echo "BasketballBund crawler failed at $(date)"
        # You can add notification logic here (email, Slack, etc.)

  health-check:
    runs-on: ubuntu-latest
    needs: crawl
    if: always()
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r crawler/requirements.txt
        
    - name: Check recent data
      env:
        SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
        SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
        LEAGUE_ID: ${{ secrets.LEAGUE_ID }}
      working-directory: ./crawler
      run: |
        python -c "import os; from supabase import create_client; from datetime import datetime, timedelta; supabase = create_client(os.getenv('SUPABASE_URL'), os.getenv('SUPABASE_KEY')); three_days_ago = datetime.utcnow() - timedelta(hours=72); result = supabase.table('scrape_log').select('*').eq('league_id', os.getenv('LEAGUE_ID')).gte('scraped_at', three_days_ago.isoformat()).execute(); exit(1) if len(result.data) == 0 else [print(f'SUCCESS: Found {len(result.data)} recent scrape(s)') or [print(f'  - {scrape[\"scraped_at\"]}: {scrape[\"teams_count\"]} teams, {scrape[\"games_count\"]} games, {scrape[\"standings_count\"]} standings') for scrape in result.data]]"
